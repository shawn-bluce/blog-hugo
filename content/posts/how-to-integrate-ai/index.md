---
title: "如何高质量地接入 AI"
slug: "how-to-integrate-ai"
date: "2025-02-06T14:20:00+0000"
lastmod: "2025-02-07T02:23:03+0000"
draft: false
tags:
  - "AI"
  - "LLM"
visibility: "public"
---
# 0X00 没有什么意义但我就是想写的前言

有一说一现在的大模型发展太快了，最开始我列这个大纲的时候是把 deepseek 作为「凑合能用但超级便宜」的一个国产替代品来介绍的，没想到过了个年它直接翻身了，现在甚至能打 GPT-4o。所以我决定现在立刻马上把这篇文章写完，否则没准又杀出来个什么模型，会导致我永远写不完了 🤣

这篇文章的主要受众群体如下：

  1. 知道最近 AI 很火，想用用看但是不知道怎么上手的朋友
  2. 尝试用过一些大模型，但是觉得接入不方便或者价格高的朋友
  3. 想全面接入 AI，让 AI 成为自己得力助手的朋友

这篇文章会介绍这些内容：

  1. 成品工具：只需要一个浏览器或者一个 APP 即可访问的最大众化的 AI 接入模式
  2. 模型选择：列举常见模型的特点，根据你的需求选择合适的大模型
  3. API 接入：不方便接入原生 Claude/OpenAI API 时的优秀替代品
  4. 三方工具：通过 API 接入后能大幅改善日常使用体验的一些工具
  5. 本地部署：有关本地部署的一些个人看法

那么废话少说，现在开始正文了

# 0X01 商业化成品工具

想要最快速的接入 AI 能力，那首选的自然是现成的商业化的 2C 产品了，所以我这里收集了几个我自己用过的比较好用的的商业化成品工具。

> 注意这部分只讨论产品，不讨论模型，有关模型的讨论放在下面一个段落。

## ChatGPT

首先，自然是目前大模型的领军人物 OpenAI 了，它的 ChatGPT 目前应该是全球范围内最知名的 AI 工具了。目前 ChatGPT 提供了多种模型可选，也提供了多模态模型和联网搜索功能，部分高级模型需要付费使用，费用一般是 $20/mon。

> 在 PC/Mac/Android 上使用 ChatGPT 需要有正确的上网姿势，在 iPhone 上还需要一个海外 Apple ID。并且注册起来也比较麻烦。如果切实需要的话可以在网上自行搜索一下注册教程。

  * 访问方式：web、app、PC/Mac 客户端
  * 优势：APP 使用体验极佳、够聪明、多模态；
  * 劣势：访问难度高、价格贵；

综合推荐等级：★★★★☆

## Claude

Claude 是追着 ChatGPT 打的另一个海外巨头，个人认为综合能力上略次于 ChatGPT 但不明显。不过同样的注册、访问都会有些麻烦。但是有一个上下文超长且比较便宜的模型（Claude-3.5-sonset-200k），可以让他阅读超长的 pdf、 MS office 等文档。同样的提供了更高级的付费模型，价格为 $20/mon。

  * 访问方式：web、app、PC/Mac 客户端
  * 优势：聪明、多模态、比 ChatGPT 访问轻松一些；
  * 劣势：访问难度依然较高、价格贵；

综合推荐等级：★★★☆☆

## 豆包

豆包是字节跳动旗下的 AI 工具，目前我亲测下来说实话聪明程度明显弱于 ChatGPT/Claude，但是它强就强在免费和体验优良。首先它内置了很多个不同 prompt 的「人」，可以作为你的助手，这一点对于不会写 prompt 的新手来说很是实用。手机的 APP 还能通过语音对谈的方式和豆包交流，虽然 ChatGPT 也有这个功能，但是在国内使用豆包反应更快并且口语听起来舒服很多很多。macOS 上的客户端可以启用一些插件功能，实现划选文字进行翻译、解释等功能。

  * 访问方式：web、app、PC/Mac 客户端
  * 优势：多模态、接入容易、语音体验良好、内置 prompt、免费；
  * 劣势：不够聪明、进阶使用体验较差；

综合推荐等级：★★★★☆

> 建议所有人在手机上装一个豆包，毕竟是免费的，有事没事跟他硬聊几句都还是划算的。而且它也能联网，所以快速搜索一些确定有结果的问题也很好用。

## deepseek

deepseek 这几天可真是爆了（以至于我一直在用的超快的 API 突然变慢了 😮‍💨）。简单介绍一下 deepseek 也是国产的，他们最擅长使用超低的成本做相同的事，最近刷屏的新闻也正是他们用了 GPT-o1 大概 2% 的成本训练出来了几乎相同水平的 DeepSeek-R1 模型。目前来说 deepseek 的重心还是在技术上，所以他们的客户端/web依旧停留在「能用」的水平线上。

  * 访问方式：web、app
  * 优势：免费、多模态、接入容易、聪明；
  * 劣势：应用层做的不够好、使用人数激增导致响应变慢；

综合推荐等级：★★★★☆

## kimi

kimi 是比较早火起来的国产 AI，当时的主要卖点是联网搜索。

  * 访问方式：web、app
  * 优势：免费、多模态、接入容易、联网搜索能力强、app 体验好；
  * 劣势：不如 DeepSeek-R1 聪明；

综合推荐等级：★★★☆☆

## POE

**POE：我们不生产模型，我们只是大模型的搬运工**

POE 是一个集成了大量模型第三方应用，在这里可以付一份钱同时使用几乎所有的热门模型。例如你想同时使用 ChatGPT 和 Claude 的高级模型，觉得同时买两个会员太贵了，就可以同样花 $20 购买 POE 的会员，这样一来就可以同时访问他们了。但需要注意的是，POE 的逻辑是会员每月送你 1000000 「点数」，每次对话会根据模型的不同消耗不同的点数（一般都是够用的，我比较高强度使用都是够的）。这也是我目前唯一付了年费的 AI 订阅项。

  * 访问方式：web、app、PC/Mac 客户端
  * 优势：访问所有热门模型（包括文生图模型）
  * 劣势：付费、使用次数有限（尽管限制很宽松）

综合推荐等级：★★★★☆

## 综合

综合看下来，我可以做出下面的推荐：

  * 如果你只是想用最简单的方式体验一下AI，并且希望它足够简单易用，那就选豆包
  * 如果你想使用目前最强的免费 AI 模型，同时愿意学习 AI 的用法，那就选 deepseek
  * 如果你想接入世界先进的 AI 模型，那就考虑订阅 ChatGPT（ChatGPT 如果不订阅的话不如直接用 deepseek）
  * 如果你想体验更多热门模型，想要了解不同模型的区别和擅长的方向，也想要深入学习使用大模型，那建议订阅 POE

# 0X02 通过 API 接入

上面我们聊的都是点击即用的成熟的商业化产品，现在来讨论一下通过 API 来接入这些模型。

> 提示：如果你听不懂 `API`、`base_url`、`API KEY`、`HTTP Method`、这几个词就意味着你暂时不适合下面的内容，继续阅读下去可能会感到有些蒙圈，这是正常现象 🤣

首先我来介绍一下为什么我们需要通过 API 接入大模型，有如下几个优势：

  * 价格：例如 ChatGPT 的价格是 $20 每月，如果你只是每天对话个十次八次的，直接开通 Plus 会显得很亏，因为 ChatGPT Plus 是按月付费的。但是 API 则是按量付费，计费方式从每月固定额度变成了根据交互的 tokens 数量计算，在用量不大的情况下会更便宜；
  * 性能：虽然没有直接证据，但是我体感上各家的 2C 产品使用的服务器和 2B 的 API 使用的服务器并非同一组，手机 APP 访问已经在卡了但是通过 API 访问就还是比较快；
  * 定制：通过 API 访问可以自己调整更多的参数，例如影响记忆力和费用的 `max_tokens` 和影响输出的 `temperature` 等；
  * 体验：使用 API 访问即使只是使用最基础的对话功能，我们也有多个前端工具可以选择，不会像官方的客户端一样没有半点定制化空间；
  * 扩展：我们可以将 API KEY 配置到很多支持的工具上，让工具们获得 AI 的能力加持（例如 obsidian、Firefox/Chrome 等）；

## 模型选择

目前通过 API 接入的话我个人只推荐 deepseek、GPT、Claude 这三个。下面是每个家族的代表和他们的能力（主观评价）和价格的对比。

名称 | 能力（5分制） | 上下文 | 输入价格（每百万 Tokens） | 输出价格（每百万 Tokens） | 备注
---|---|---|---|---|---
DeepSeek V3 | 3 | 64k | $0.27 | $1.10 | 价格无敌，配置在浏览器扩展上用于翻译和总结非常合适
DeepSeek R1 | 4.5 | 64k | $0.55 | 2.19 | 性价比无敌，能力媲美贵它将近 30 倍的 GPT-o1
GPT-4o | 3.5 | 128k | $2.5 | $10 | \---
GPT-o1 | 4.5 | 200k | $15 | $60 | \---
Claude-3.5-sonset | 4 | 200k | $3 | $15 | \---

目前我自己的体验下来，推荐如下：

  * DeepSeek V3 价格最便宜，充 10 块钱可以用很久。适合将它配置到一些支持 AI 的工具上，用来总结文章、大段翻译等。虽然它不够聪明但是足够便宜，随便调用也不用心疼自己的钱包；
  * DeepSeek R1 目前使用体验良好，足够聪明价格也是很便宜。如果不嫌弃它每次都要思考半天的话（其实这是它的优势项），可以作为主力 AI 模型使用，它理论上更擅长数学、编程等逻辑性强的工作；
  * GPT-4o 是传统模型中很强的了，没有 o1 和 R1 的推理过程，反应比较迅速。价格虽然比较贵但多少能承受，也是主力 AI 模型的一个优秀备选；
  * GPT-o1 应该是这些模型里理论最强的，但是这个价格嘛也很离谱。如果你在意它比 R1 强的那一点能力且不在意这 30 倍的价差，那 GPT-o1 是个不错的选择；
  * Claude-3.5-sonset 最大的优势是 200k 的上下文，是目前热门模型中上下文窗口最大的一个。如果你经常有大上下文的需求那么 Claude-3.5-sonset 是一个优秀的选择；

> 有关 tokens 和上下文：
>
> 我们在和大模型交互的时候，我们自己说的文字和大模型返回的文字都会被分词，然后将分词之后的结果作为 token 计算。例如我给模型提供一个 10 万个汉字的文档，让他回答我的一个问题，AI 的回答大概是 1000 个汉字的话，大约会消耗掉 10 万个 tokens。
>
> 上下文则是我们和模型对话时 AI 能处理的最大 tokens 数量。上面的例子一次对话就需要消耗 10 万个 tokens 也就是 100k 左右，也就意味着 DeepSeek V3/R1 都无法处理这个对话请求，但是 GPT-4o/o1 和 Claude-3.5-sonset 都能处理。

## 如何接入

众所周知，接入 AWS 的 API 就需要 AWS 的账号，但是现在的 AI 接入方式多少有些不一样的地方。当然通过 OpenAI 官方接入 OpenAI 的模型是天经地义的，但是我们国内用户光是注册 OpenAI 和 Claude 的账号就已经很费劲了，后面付费还有一座大山拦路，着实是整不动。所以在官方接入的传统方式之外还可以通过转发站点实现 API 接入，这种接入方式不仅注册和付费更轻松，还会有些许优惠。这里介绍两个站点：[头顶冒火](<https://burn.hair/>) 和 [302.ai](<https://gpt302.saaslink.net/M8oaa5>)，这两个平台类似，都是类似于前面介绍的 POE 的平台，只不过这两个平台提供的是聚合的 API 服务，在这一处充值后能通过 API 访问平台支持的模型。

如果你想通过 API 的方式接入 DeepSeek 的话就不需要再用上面的服务中转了，直接去官网注册账号然后支付宝微信付款用就行了。

> 另外有个好消息，目前已知的绝大多数大模型的 API 都兼容 OpenAI 的 API 标准，也就意味着绝大多数能使用 OpenAI API 的工具都能通过修改 `base_ur`, `API KEY`, `model` 这三个参数的方式快速接入。

# 0X03 第三方工具

使用第三发工具的前提是通过 API 访问大模型，个人认为这才是当前大模型的完整使用方式。下面介绍几个自己真正用过的第三方工具：

## glarity

这是一个浏览器扩展，配置好 API 之后它会有一个悬浮按钮在浏览器里，可以一键快速调用大模型对当前页面进行总结，也可以针对当前页面进行提问，还可以做到在尽力维持页面布局的情况下实现逐行翻译。强烈推荐所有可以通过 API 访问大模型的朋友安装试用。

## ChatGPT-Next-Web

这是一个简单的聊天工具，不管是 DeepSeek 还是 ChatGPT 他们都有自己的页面，但是通过 API 访问的话总不能就在命令行里用吧。所以 ChatGPT-Next-Web 就出现了，它就是一个可以创建多个对话也能上传文件的前端工具，给它配好 API 就能像普通客户端一样使用 API 了。值得表扬的是这个开源项目可以部署在服务器上，通过浏览器访问，效果非常棒。也就意味着你可以自己开通 API 后把它部署在公网服务器上，配置一个密码，就能让自己和家人朋友一起用了。

## Obsidian

如果你也用 Obsidian 的话可以给你推荐一个名为 Text Generator 的插件，这个插件配置好之后可以在 Obsidian 中调用 API。可以实现的功能包括但不限于：选中大段文字让他检查错误、总结整篇笔记、生成某某的介绍等。虽然不如 Notion 的 Notion AI 集成度那么高，但是体验也还是不错的。

# 0X04 本地化部署

最后再聊一聊本地部署吧，最开始的时候我尝试过在 MacBook 上部署 Llama 3.1 的 7B 和 14B 模型测试。最近 DeepSeek R1 的突然爆火也开始出现了大量的本地部署的教程，给人一种在本地部署之后就不需要联网使用官方服务的感觉。

我自己测试的硬件配置是一台 M2 Max 32G 的 MacBook Pro，按理说跑大模型的性能应该是超出大多数朋友的电脑的。就以最近我测试的 DeepSeek R1 为例，简单测试的结果如下：

模型 | 效果 | 速度
---|---|---
DeepSeek R1 7B | 一般 | 飞快
DeepSeek R1 14B | 一半 | 飞快
DeepSeek R1 32B | 可以非严肃环境使用 | 比较快
DeepSeek R1 70B | 跑不动 | 跑不动

并且即使我跑 32B 这样的模型，效果也并不够看。所以我自己的个人看法就是：

  * 如果你只能跑 7B 或者 14B 的模型，那可以用来学习、体验、图一乐
  * 如果你能跑 32B 的模型，那可以用来正经聊聊，但是能力依旧有限，不适合严肃环境使用
  * 如果你能跑 70B 的模型，那......我没试过，不知道能力怎么样 🤣
  * 如果你能跑 671B 的模型，那请你联系我，我想和土豪做朋友

如果要在本地部署的话，目前比较推荐的就只有 DeepSeek 和 Llama 了，并且在模型选择上基本 32B 就到头了，如果是自己本地跑跑玩玩的话建议从 7B 和 14B 中进行选择，最多最多试试 70B（如果是 Mac 的融合内存的话）。

> 最后悄悄说一下，本地部署的话可以了解一下带有 `abliterated` 标签的模型，这才是本地部署模型最大的意义。
